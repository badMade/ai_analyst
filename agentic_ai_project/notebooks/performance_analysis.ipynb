{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Analysis Notebook\n",
    "\n",
    "Analyze and compare agent performance across different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from src.agents import AutonomousAgent, LearningAgent, ReasoningAgent\n",
    "from src.environment import Simulator, EnvironmentConfig\n",
    "from src.utils import MetricsCollector, Visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Benchmark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark parameters\n",
    "NUM_EPISODES = 10\n",
    "MAX_STEPS = 100\n",
    "SEED = 42\n",
    "\n",
    "# Create agents to compare\n",
    "agents = {\n",
    "    'Autonomous': AutonomousAgent(name='Auto'),\n",
    "    'Learning': LearningAgent(name='Learn'),\n",
    "    'Reasoning': ReasoningAgent(name='Reason'),\n",
    "}\n",
    "\n",
    "print(f\"Comparing {len(agents)} agent types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "results = {name: [] for name in agents.keys()}\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    for name, agent in agents.items():\n",
    "        env = Simulator(\n",
    "            config=EnvironmentConfig(max_steps=MAX_STEPS, seed=SEED + episode)\n",
    "        )\n",
    "        observation, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(MAX_STEPS):\n",
            # Agent perceives and decides on an action
            perception = agent.perceive(observation)
            agent_decision = agent.decide(perception)
            action = agent_decision.action_type
            
            # Environment executes the agent's chosen action
            result = env.step(action)
    "            episode_reward += result.reward\n",
    "            observation = result.observation\n",
    "            \n",
    "            if result.terminated or result.truncated:\n",
    "                break\n",
    "        \n",
    "        results[name].append(episode_reward)\n",
    "        agent.reset()\n",
    "\n",
    "print(\"Benchmarks complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "viz = Visualizer()\n",
    "\n",
    "# Calculate statistics\n",
    "stats = {}\n",
    "for name, rewards in results.items():\n",
    "    stats[name] = {\n",
    "        'mean': statistics.mean(rewards),\n",
    "        'std': statistics.stdev(rewards) if len(rewards) > 1 else 0,\n",
    "        'max': max(rewards),\n",
    "        'min': min(rewards),\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "print(\"Performance Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Bar chart of mean rewards\n",
    "mean_rewards = {name: s['mean'] for name, s in stats.items()}\n",
    "print(\"\\nMean Rewards:\")\n",
    "print(viz.bar_chart(mean_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detailed Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "table_data = [\n",
    "    {\n",
    "        'Agent': name,\n",
    "        'Mean': f\"{s['mean']:.2f}\",\n",
    "        'Std': f\"{s['std']:.2f}\",\n",
    "        'Max': f\"{s['max']:.2f}\",\n",
    "        'Min': f\"{s['min']:.2f}\",\n",
    "    }\n",
    "    for name, s in stats.items()\n",
    "]\n",
    "\n",
    "print(\"\\nDetailed Statistics:\")\n",
    "print(viz.table(table_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Episode-by-Episode Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEpisode Rewards Sparklines:\")\n",
    "for name, rewards in results.items():\n",
    "    sparkline = viz.sparkline(rewards, width=30)\n",
    "    print(f\"  {name:12s}: {sparkline}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
