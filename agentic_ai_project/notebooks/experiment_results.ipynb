{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Results Notebook\n",
    "\n",
    "Document and analyze experimental results from agent training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from src.utils import MetricsCollector, Visualizer\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = {\n",
    "    'name': 'Agent Comparison Study',\n",
    "    'date': datetime.now().isoformat(),\n",
    "    'description': 'Comparing different agent architectures on navigation task',\n",
    "    'parameters': {\n",
    "        'episodes': 100,\n",
    "        'max_steps': 200,\n",
    "        'environment': 'Simulator',\n",
    "        'world_size': (100, 100),\n",
    "        'num_resources': 15,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Experiment: {experiment['name']}\")\n",
    "print(f\"Date: {experiment['date']}\")\n",
    "print(f\"Description: {experiment['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Results Data\n",
    "\n",
    "In practice, these would be loaded from saved experiment files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample experiment results\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Simulated results from multiple runs\n",
    "experiment_results = {\n",
    "    'autonomous_agent': {\n",
    "        'rewards': [random.gauss(50, 10) for _ in range(100)],\n",
    "        'steps': [random.randint(80, 200) for _ in range(100)],\n",
    "        'success_rate': 0.72,\n",
    "    },\n",
    "    'learning_agent': {\n",
    "        'rewards': [random.gauss(45 + i*0.3, 8) for i in range(100)],\n",
    "        'steps': [random.randint(70, 180) for _ in range(100)],\n",
    "        'success_rate': 0.68,\n",
    "    },\n",
    "    'reasoning_agent': {\n",
    "        'rewards': [random.gauss(55, 12) for _ in range(100)],\n",
    "        'steps': [random.randint(90, 190) for _ in range(100)],\n",
    "        'success_rate': 0.75,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"Loaded results for {len(experiment_results)} agents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "viz = Visualizer()\n",
    "\n",
    "# Analyze each agent\n",
    "for agent_name, data in experiment_results.items():\n",
    "    rewards = data['rewards']\n",
    "    \n",
    "    print(f\"\\n### {agent_name.replace('_', ' ').title()} ###\")\n",
    "    print(f\"  Mean Reward: {statistics.mean(rewards):.2f}\")\n",
    "    print(f\"  Std Dev: {statistics.stdev(rewards):.2f}\")\n",
    "    print(f\"  Success Rate: {data['success_rate']:.0%}\")\n",
    "    print(f\"  Progress: {viz.sparkline(rewards[:20], width=20)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Success rate comparison\n",
    "success_rates = {\n",
    "    name.replace('_', ' ').title(): data['success_rate'] * 100\n",
    "    for name, data in experiment_results.items()\n",
    "}\n",
    "\n",
    "print(\"Success Rate Comparison:\")\n",
    "print(viz.bar_chart(success_rates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show learning curve for learning agent\n",
    "learning_rewards = experiment_results['learning_agent']['rewards']\n",
    "\n",
    "print(\"Learning Agent - Reward Over Time:\")\n",
    "print(viz.line_chart(\n",
    "    learning_rewards[:50],\n",
    "    config=viz.config,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple comparison of means\n",
    "all_means = {\n",
    "    name: statistics.mean(data['rewards'])\n",
    "    for name, data in experiment_results.items()\n",
    "}\n",
    "\n",
    "best_agent = max(all_means, key=all_means.get)\n",
    "print(f\"\\nBest performing agent: {best_agent}\")\n",
    "print(f\"Mean reward: {all_means[best_agent]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Based on this experiment:\n",
    "1. The reasoning agent achieved the highest success rate\n",
    "2. The learning agent showed improvement over time\n",
    "3. All agents performed within expected parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
