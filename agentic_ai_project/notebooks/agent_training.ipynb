{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Training Notebook\n",
    "\n",
    "This notebook demonstrates training agents in the Agentic AI framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from src.agents import LearningAgent, AutonomousAgent\n",
    "from src.environment import Simulator, EnvironmentConfig\n",
    "from src.utils import MetricsCollector, Visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training environment\n",
    "env_config = EnvironmentConfig(\n",
    "    max_steps=200,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "env = Simulator(config=env_config, num_resources=15)\n",
    "print(f\"Environment created: {env.world_size}\")\n",
    "print(f\"Available actions: {env.get_action_space()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Agent Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agents.learning_agent import LearningConfig\n",
    "\n",
    "agent_config = LearningConfig(\n",
    "    learning_rate=0.001,\n",
    "    batch_size=32,\n",
    "    discount_factor=0.99,\n",
    ")\n",
    "\n",
    "agent = LearningAgent(name=\"TrainingAgent\", config=agent_config)\n",
    "print(f\"Agent initialized: {agent.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agents.base_agent import AgentAction\n",
    "import random\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 20\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.95\n",
    "epsilon_min = 0.1\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        # Epsilon-greedy action\n",
    "        if random.random() < epsilon:\n",
            else:
                # Use agent's learned policy
                perception = agent.perceive(observation.to_array())
                decision = agent.decide(perception)
                action_idx = decision.parameters.get("action_index", 0)
    "        else:\n",
    "            action_idx = 0  # Placeholder for learned policy\n",
    "        \n",
    "        action = env.ACTIONS[action_idx]\n",
    "        result = env.step(action)\n",
    "        \n",
    "        # Learn from experience\n",
    "        agent.learn(\n",
    "            state=observation.to_array(),\n",
    "            action=AgentAction(action, {\"action_index\": action_idx}),\n",
    "            reward=result.reward,\n",
    "            next_state=result.observation.to_array(),\n",
    "            done=result.terminated or result.truncated,\n",
    "        )\n",
    "        \n",
    "        episode_reward += result.reward\n",
    "        observation = result.observation\n",
    "        \n",
    "        if result.terminated or result.truncated:\n",
    "            break\n",
    "    \n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    episode_rewards.append(episode_reward)\n",
    "    \n",
    "    if (episode + 1) % 5 == 0:\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "print(\"Training Statistics:\")\n",
    "print(f\"  Mean reward: {statistics.mean(episode_rewards):.2f}\")\n",
    "print(f\"  Max reward: {max(episode_rewards):.2f}\")\n",
    "print(f\"  Min reward: {min(episode_rewards):.2f}\")\n",
    "\n",
    "# Visualize progress\n",
    "viz = Visualizer()\n",
    "print(\"\\nReward Progression:\")\n",
    "print(viz.sparkline(episode_rewards, width=40))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
